<div align="center" id="moerltop">
<img src="https://raw.githubusercontent.com/slowfastai/moerl/main/docs/assets/logo.png" alt="logo" width="400" margin="10px"></img>

<h3 align="center">
An efficient RL fine-tuning framework for Mixture of Experts models
</h3>

</div>


<br/><br/>

## ğŸŒŸ Acknowledgements

### ğŸ’¡ Inspiration and Foundation

MoERL was sparked by the brilliant work of the [Unsloth](https://github.com/unslothai/unsloth) project, developed by Daniel Han-Chen & the Unsloth team ğŸ¦¥âœ¨  
Their innovative approach to LLM optimization and commitment to open-source gave us a huge boost of inspiration ğŸ’¡

While MoERL explores a different pathâ€”focusing on Mixture of Experts (MoE) and reinforcement learningâ€”we share the same passion for efficient, accessible, and extensible LLMs ğŸ¯ğŸ”§


- **Repository:** ğŸ¦¥ [Unsloth GitHub](https://github.com/unslothai/unsloth) &nbsp; <img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" height="50" align="center" />
- **Creator:** Daniel Han-Chen & Unsloth Team
- **License:** Apache License, Version 2.0


<br/><br/>
Weâ€™re deeply thankful for the open-source ecosystem that empowers us to learn, remix, and build better toolsâ€”together ğŸ’ğŸš€

### â¤ï¸ Additional Acknowledgements


MoERL stands on the shoulders of open-source giants ğŸ¦¾. We extend our heartfelt thanks to the incredible communities and tools that have laid the foundation for our work in the field of large language models and reinforcement learning:
- **[ğŸ¤— Transformers](https://github.com/huggingface/transformers)**: For providing a powerful and flexible ecosystem for working with state-of-the-art NLP models.
- <img src="https://avatars.githubusercontent.com/u/175231607?s=200&v=4" height="20" align="center"> **[BitsAndBytes](https://github.com/bitsandbytes-foundation/bitsandbytes)**: For enabling memory-efficient quantization and optimization.
- **[ğŸ¤— PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)**: For making parameter-efficient techniques easy to integrate.
- **[ğŸ¤— TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)**: For providing reinforcement learning tools tailored to language models.
- <img src="https://avatars.githubusercontent.com/u/136984999?s=200&v=4" height="20" align="center"> **[vLLM](https://github.com/vllm-project/vllm)**: For enabling fast and efficient LLM inference.

Your amazing work helps the entire ecosystem go further, faster, and more fun ğŸ’ªğŸŒ  

Weâ€™re excited to contribute back and co-create the future of LLMs together!

## ğŸ“„ NOTICE: Use of LGPLv3 Dependencies in MoERL

MoERL is licensed under the Apache License, Version 2.0. However, this project depends on the following third-party libraries that are licensed under the GNU Lesser General Public License (LGPL) Version 3.0:

### ğŸ“¦ LGPLv3 Dependencies

- **Library Name**: `unsloth-zoo`  
- **License**: GNU Lesser General Public License v3.0 (LGPL-3.0)  
- **Source Repository**: [https://github.com/unslothai/unsloth-zoo](https://github.com/unslothai/unsloth-zoo)  
- **Usage in MoERL**: Dynamically imported at runtime (no source code copied).

### âœ… Legal Position

MoERL uses this library **only through dynamic runtime imports** and does **not contain any part of the LGPLv3-licensed code**. According to the terms of LGPLv3, this form of use does **not constitute a derivative work**, and therefore does not require MoERL itself to adopt the LGPLv3 license.

So MoERL happily stays Apache License 2.0 while still interoperating with awesome LGPL tools ğŸ§¡

Users of this software are hereby informed of the existence of these LGPLv3 components. 
If you want to inspect or use `unsloth-zoo`, you can find its source code at the link above ğŸ”—

### ğŸ”— References & Licensing

- ğŸ¦¥ [unsloth-zoo](https://github.com/unslothai/unsloth-zoo) 
- ğŸ“š [GNU LGPLv3 License (official)](https://www.gnu.org/licenses/lgpl-3.0.html)  [SPDX Reference](https://spdx.org/licenses/LGPL-3.0-only.html)
- ğŸ“œ [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)  
  

For questions, licensing love letters ğŸ’Œ, or clarifications, feel free to contact the MoERL maintainers ğŸ˜„
