<div align="center" id="moerltop">
<img src="https://raw.githubusercontent.com/slowfastai/moerl/main/docs/assets/logo.png" alt="logo" width="400" margin="10px"></img>

<h3 align="center">
An efficient RL fine-tuning framework for Mixture of Experts models
</h3>

</div>


<br/><br/>

## 🌟 Acknowledgements

### 💡 Inspiration and Foundation

MoERL was sparked by the brilliant work of the [Unsloth](https://github.com/unslothai/unsloth) project, developed by Daniel Han-Chen & the Unsloth team 🦥✨  
Their innovative approach to LLM optimization and commitment to open-source gave us a huge boost of inspiration 💡

While MoERL explores a different path—focusing on Mixture of Experts (MoE) and reinforcement learning—we share the same passion for efficient, accessible, and extensible LLMs 🎯🔧


- **Repository:** 🦥 [Unsloth GitHub](https://github.com/unslothai/unsloth) &nbsp; <img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" height="50" align="center" />
- **Creator:** Daniel Han-Chen & Unsloth Team
- **License:** Apache License, Version 2.0


<br/><br/>
We’re deeply thankful for the open-source ecosystem that empowers us to learn, remix, and build better tools—together 💞🚀

### ❤️ Additional Acknowledgements


MoERL stands on the shoulders of open-source giants 🦾. We extend our heartfelt thanks to the incredible communities and tools that have laid the foundation for our work in the field of large language models and reinforcement learning:
- **[🤗 Transformers](https://github.com/huggingface/transformers)**: For providing a powerful and flexible ecosystem for working with state-of-the-art NLP models.
- <img src="https://avatars.githubusercontent.com/u/175231607?s=200&v=4" height="20" align="center"> **[BitsAndBytes](https://github.com/bitsandbytes-foundation/bitsandbytes)**: For enabling memory-efficient quantization and optimization.
- **[🤗 PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)**: For making parameter-efficient techniques easy to integrate.
- **[🤗 TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)**: For providing reinforcement learning tools tailored to language models.
- <img src="https://avatars.githubusercontent.com/u/136984999?s=200&v=4" height="20" align="center"> **[vLLM](https://github.com/vllm-project/vllm)**: For enabling fast and efficient LLM inference.

Your amazing work helps the entire ecosystem go further, faster, and more fun 💪🌍  

We’re excited to contribute back and co-create the future of LLMs together!

## 📄 NOTICE: Use of LGPLv3 Dependencies in MoERL

MoERL is licensed under the Apache License, Version 2.0. However, this project depends on the following third-party libraries that are licensed under the GNU Lesser General Public License (LGPL) Version 3.0:

### 📦 LGPLv3 Dependencies

- **Library Name**: `unsloth-zoo`  
- **License**: GNU Lesser General Public License v3.0 (LGPL-3.0)  
- **Source Repository**: [https://github.com/unslothai/unsloth-zoo](https://github.com/unslothai/unsloth-zoo)  
- **Usage in MoERL**: Dynamically imported at runtime (no source code copied).

### ✅ Legal Position

MoERL uses this library **only through dynamic runtime imports** and does **not contain any part of the LGPLv3-licensed code**. According to the terms of LGPLv3, this form of use does **not constitute a derivative work**, and therefore does not require MoERL itself to adopt the LGPLv3 license.

So MoERL happily stays Apache License 2.0 while still interoperating with awesome LGPL tools 🧡

Users of this software are hereby informed of the existence of these LGPLv3 components. 
If you want to inspect or use `unsloth-zoo`, you can find its source code at the link above 🔗

### 🔗 References & Licensing

- 🦥 [unsloth-zoo](https://github.com/unslothai/unsloth-zoo) 
- 📚 [GNU LGPLv3 License (official)](https://www.gnu.org/licenses/lgpl-3.0.html)  [SPDX Reference](https://spdx.org/licenses/LGPL-3.0-only.html)
- 📜 [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)  
  

For questions, licensing love letters 💌, or clarifications, feel free to contact the MoERL maintainers 😄
